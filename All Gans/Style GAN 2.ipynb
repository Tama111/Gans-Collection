{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5ec240",
   "metadata": {},
   "source": [
    "# Style Gan 2\n",
    "\n",
    "This is an attempt to re-implement the paper Style-GAN 2\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1912.04958.pdf\n",
    "\n",
    "Other Resources:\n",
    "* https://github.com/NVlabs/stylegan2\n",
    "* https://nn.labml.ai/gan/stylegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afc74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStddev(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Mini Batch Standard Deviation\n",
    "    \n",
    "    Args:\n",
    "        epsilon (float): small value to avoid division by 0. | default -> 1e-8\n",
    "        \n",
    "    Input: (Feature Maps)\n",
    "        Input is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, feature_maps)\n",
    "        \n",
    "    Output: (Feature Maps)\n",
    "        Output is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, feature_maps + 1)\n",
    "        \n",
    "        extra feature is added which is the mini batch standard deviation (along axis 0)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, epsilon: float = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inp_shp = tf.shape(inputs)\n",
    "        \n",
    "        mean = tf.reduce_mean(inputs, keepdims = True, axis = 0)\n",
    "        std = tf.sqrt(tf.math.reduce_mean(tf.square(inputs - mean), keepdims = True, axis = 0) + self.epsilon)\n",
    "        avg_std = tf.reduce_mean(std, keepdims = True)\n",
    "        tiled = tf.tile(avg_std, (inp_shp[0], inp_shp[1], inp_shp[2], 1))\n",
    "        combined = tf.concat([inputs, tiled], axis = -1)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f006e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Linear Layer\n",
    "    \n",
    "    Args:\n",
    "        neurons (int): Define the number of neurons in the layer here.\n",
    "        gain (float): Define gain here for weight scaling. | default -> np.sqrt(2)\n",
    "        \n",
    "    Inputs: (Linear)\n",
    "        Input is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, input_neurons)\n",
    "        \n",
    "    Outputs: (Linear)\n",
    "        output is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, neurons)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, neurons: int, gain: float = np.sqrt(2), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.neurons = neurons\n",
    "        self.gain = gain\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        inp_neurons = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (inp_neurons, self.neurons), initializer = init, \n",
    "                                 trainable = True, name = 'Weight')\n",
    "        self.B = self.add_weight(shape = (self.neurons, ), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'Bias')\n",
    "        \n",
    "        self.w_scale = self.gain * tf.math.rsqrt(tf.cast(inp_neurons, tf.float32))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.matmul(inputs, self.W * self.w_scale), self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d26cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Convolution 2-Dimensional Layer\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        kernel_size (tuple): Define the size of kernel here. | default-> (3, 3)\n",
    "        strides (tuple): Define the size of strides here. | default -> (1, 1)\n",
    "        gain (float): Define gain here for weight scaling. | default -> np.sqrt(2)\n",
    "    \n",
    "    Inputs: (Feature Maps)\n",
    "        Input is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, feature_maps)\n",
    "        \n",
    "    Outputs: (Feature Maps)\n",
    "        Output is a single 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, filters)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, filters: int, kernel_size: tuple = (3, 3), strides: tuple = (1, 1), gain: float = 1e-8, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.gain = gain\n",
    "        self.padding = 'SAME' if (kernel_size[0]-1)//2 else 'VALID'\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        inp_chn = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (self.kernel_size[0], self.kernel_size[1], inp_chn, self.filters), \n",
    "                                 initializer = init, trainable = True, name = 'Weight')\n",
    "        self.B = self.add_weight(shape = (self.filters, ), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'Bias')\n",
    "        \n",
    "        fan_in = tf.cast(self.kernel_size[0] * self.kernel_size[1] * inp_chn, tf.float32)\n",
    "        self.w_scale = self.gain * tf.math.rsqrt(fan_in)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.nn.conv2d(inputs, self.W * self.w_scale, self.strides, self.padding, 'NHWC'), self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497e9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DMod(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Convolutional 2-Dimensional Layer with Modulated & Demodulated Weights\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        kernel_size (tuple): Define the size of kernel here. | default-> (3, 3)\n",
    "        strides (tuple): Define the size of strides here. | default -> (1, 1)\n",
    "        demodulate (bool): Define whether to demodulate the weights. | default -> True\n",
    "        epsilon (float): small value to avoid division by 0. | default -> 1e-8\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimension indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Mapped Output`: 2-Dimensional Tensor. Each dimension indicating (batch_size, latent_dim)\n",
    "            \n",
    "    Outputs: (Feature Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, filters)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, filters: int, kernel_size: tuple = (3, 3), strides: tuple = (1, 1), demodulate: bool = True, \n",
    "                 epsilon: float = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = 'SAME' if (kernel_size[0]-1)//2 else 'VALID'\n",
    "        self.demodulate = demodulate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def build(self, input_shapes):\n",
    "        inp_chn = input_shapes[0][-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (self.kernel_size[0], self.kernel_size[1], inp_chn, self.filters), \n",
    "                                 initializer = init, trainable = True, name = 'Weight')\n",
    "        self.linear = Linear(neurons = inp_chn)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, m = inputs\n",
    "                \n",
    "        s = self.linear(m)[:, tf.newaxis, tf.newaxis, :, tf.newaxis]\n",
    "        w = tf.expand_dims(self.W, axis = 0)\n",
    "        w *= s\n",
    "        \n",
    "        if self.demodulate:\n",
    "            w *= tf.math.rsqrt(tf.math.reduce_sum(tf.square(w), keepdims = True, axis = [1, 2, 3]) + self.epsilon)\n",
    "            \n",
    "        x = tf.transpose(x, perm = [0, 3, 1, 2])\n",
    "        x = tf.reshape(x, (1, -1, tf.shape(x)[2], tf.shape(x)[3]))\n",
    "        \n",
    "        w = tf.transpose(w, perm = [1, 2, 3, 0, 4])\n",
    "        w = tf.reshape(w, (tf.shape(w)[0], tf.shape(w)[1], tf.shape(w)[2], -1))\n",
    "        \n",
    "        out = tf.nn.conv2d(x, w, self.strides, self.padding, 'NCHW')\n",
    "        \n",
    "        out = tf.reshape(out, (-1, self.filters, tf.shape(out)[2], tf.shape(out)[3]))\n",
    "        out = tf.transpose(out, perm = [0, 2, 3, 1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25eb21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Add Noise\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Noise`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, 1)\n",
    "            \n",
    "    Output: (Feature Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimensions indicating \n",
    "        (batch_size, height, width, feature_map)\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        inp_chn = input_shape[0][-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (1, 1, 1, inp_chn), initializer = init, \n",
    "                                 trainable = True, name = 'Noise_Weight')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(inputs[0], tf.multiply(self.W, inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1234433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Add Bias\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Inputs: (Feature Maps)\n",
    "        Input is a 4-Dimensional Tensor. Each dimensions indicating \n",
    "        (batch_size, height, width, feature_map)\n",
    "        \n",
    "    Outputs: (Feature Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimensions indicating\n",
    "        (batch_size, height, width, feature_map)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        inp_chn = input_shape[-1]\n",
    "        self.B = self.add_weight(shape = (1, 1, 1, inp_chn), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'Bias')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(inputs, self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a7c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRGB(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Convert To RGB Image (with 3 channels)\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Latent Dimension`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "        \n",
    "    Outputs:\n",
    "        Output is a 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, 3)\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = Conv2DMod(filters = 3, kernel_size = (1, 1), strides = (1, 1), demodulate = False)\n",
    "        self.act = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.add_bias = AddBias()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.act(self.add_bias(self.conv(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd216521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromRGB(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = Conv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1), gain = 1.0)\n",
    "        self.act = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.act(self.conv(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635c6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleBlock(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Style Block\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Mapped Output`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "            (3) `Noise`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, 1)\n",
    "            \n",
    "    Outputs: (Feature Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimension indicating\n",
    "        (batch_size, height, width, filters)\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, filters: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = Conv2DMod(filters = filters, kernel_size = (3, 3), strides = (1, 1), demodulate = True)\n",
    "        self.add_noise = AddNoise()\n",
    "        self.add_bias = AddBias()\n",
    "        self.act = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, m, n = inputs\n",
    "        return self.act(self.add_bias(self.add_noise([self.conv([x, m]), n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9c1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGeneratorBlock(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Skip Generator Block\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Mapped Output`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "            (3) `Noise`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "            (4) `Previous RGB Output`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, 3)\n",
    "            \n",
    "    Outputs:\n",
    "        Outputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size*2, height, width*2, filters)\n",
    "            (2) `RGB`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height*2, width*2, 3)\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, filters: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.up_sample = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'bilinear')\n",
    "        self.up_sample_rgb = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'bilinear')\n",
    "        \n",
    "        self.style_block_1 = StyleBlock(filters = filters)\n",
    "        self.style_block_2 = StyleBlock(filters = filters)\n",
    "        \n",
    "        self.to_rgb = ToRGB()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x, m, n, prev_rgb = inputs\n",
    "        \n",
    "        prev_rgb = self.up_sample_rgb(prev_rgb)\n",
    "        \n",
    "        out = self.up_sample(x)\n",
    "        out = self.style_block_1([out, m, n])\n",
    "        out = self.style_block_2([out, m, n])\n",
    "        \n",
    "        rgb = self.to_rgb([out, m])\n",
    "        next_rgb = tf.add(rgb, prev_rgb)\n",
    "        \n",
    "        return out, next_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba499e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipDiscriminatorBlock(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Skip Discriminator Block\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        \n",
    "    Inputs: \n",
    "        Inputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_maps)\n",
    "            \n",
    "    Outputs:\n",
    "        Outputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height//2, width//2, filters)\n",
    "            (2) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height//2, width//2, filters)\n",
    "    '''\n",
    "    def __init__(self, filters: int, first_block: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.first_block = first_block\n",
    "        \n",
    "        self.from_rgb = FromRGB(filters = filters)\n",
    "        self.conv_1 = Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))\n",
    "        self.act_1 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.conv_2 = Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))\n",
    "        self.act_2 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        if not self.first_block:\n",
    "            self.down_sample_d = tf.keras.layers.AveragePooling2D()\n",
    "            self.conv_d = Conv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1))\n",
    "            self.act_d = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.down_sample_1 = tf.keras.layers.AveragePooling2D()\n",
    "        self.down_sample_2 = tf.keras.layers.AveragePooling2D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        img = inputs\n",
    "\n",
    "        out_1 = self.down_sample_1(img[0])\n",
    "        x1 = self.from_rgb(img[0])\n",
    "        x1 = self.act_1(self.conv_1(x1))\n",
    "        x1 = self.act_2(self.conv_2(x1))\n",
    "        x1 = self.down_sample_2(x1)\n",
    "        if not self.first_block:\n",
    "            x2 = self.down_sample_d(img[1])\n",
    "            x2 = self.act_d(self.conv_d(x2))\n",
    "            out_2 = tf.add(x1, x2)\n",
    "        else:\n",
    "            out_2 = x1\n",
    "\n",
    "        return out_1, out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e542799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1de5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGeneratorBlock(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Residual Generator Block\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        \n",
    "    Inputs:\n",
    "        Inputs are :-\n",
    "            (1) `Feature Maps`: 4-Dimensional Tensor. Each dimensions indicating (batch_size, height, width, feature_map)\n",
    "            (2) `Mapped Output`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "            (3) `Noise`: 2-Dimensional Tensor. Each dimensions indicating (batch_size, latent_dim)\n",
    "        \n",
    "    Outputs: (Feature Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimensions indicating (batch_size, height*2, width*2, filters)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, filters: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.up_sample_1 = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'bilinear')\n",
    "        self.up_sample_2 = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'bilinear')\n",
    "        \n",
    "        self.style_block_1 = StyleBlock(filters = filters)\n",
    "        self.style_block_2 = StyleBlock(filters = filters)\n",
    "        \n",
    "        #self.conv = Conv2DMod(filters = filters, kernel_size = (1, 1), strides = (1, 1), demodulate = False)\n",
    "        self.conv = Conv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1), gain = 1.0)\n",
    "        self.act = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, m, n = inputs\n",
    "        \n",
    "        x1 = self.up_sample_1(x)\n",
    "        x1 = self.act(self.conv([x1, m]))\n",
    "        \n",
    "        x2 = self.up_sample_2(x)\n",
    "        x2 = self.style_block_1([x2, m, n])\n",
    "        x2 = self.style_block_2([x2, m, n])\n",
    "        \n",
    "        out = tf.add(x1, x2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eaa4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDiscriminatorBlock(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Residual Discriminator Block\n",
    "    \n",
    "    Args:\n",
    "        filters (int): Define the number of output feature_maps/filters here.\n",
    "        \n",
    "    Inputs: (Feature Maps)\n",
    "        Input is a 4-Dimensional Tensor. Each dimensions indicating \n",
    "        (batch_size, height, width, feature_map)\n",
    "        \n",
    "    Outputs: (Featute Maps)\n",
    "        Output is a 4-Dimensional Tensor. Each dimensions indicating \n",
    "        (batch_size, height//2, width//2, feature_map)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, filters: float, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))\n",
    "        self.act_1 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.conv_2 = Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))\n",
    "        self.act_2 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.down_sample = tf.keras.layers.AveragePooling2D()\n",
    "        \n",
    "        self.down_sample_r = tf.keras.layers.AveragePooling2D()\n",
    "        self.conv_r = Conv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1), gain = np.sqrt(2))\n",
    "        self.act_r = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.scale = tf.math.rsqrt(2.0)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x1 = self.act_1(self.conv_1(inputs))\n",
    "        x1 = self.act_2(self.conv_2(x1))\n",
    "        x1 = self.down_sample(x1)\n",
    "        \n",
    "        x2 = self.act_r(self.conv_r(self.down_sample_r(inputs)))\n",
    "        \n",
    "        out = tf.add(x1, x2) * self.scale\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e213daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Normalize\n",
    "    \n",
    "    Args:\n",
    "        p (int): Define the exponent value in the norm formulation. | default -> 2\n",
    "        axis (int): Define the dimension to reduce. | default -> 1\n",
    "        epsilon (float): Define small value to avoid division by zero. | default -> 1e-8\n",
    "        \n",
    "    Inputs: (Linear)\n",
    "        Input is a 2-Dimensional Tensor. Each dimensions indicating\n",
    "        (batch_size, latent_dim)\n",
    "        \n",
    "    Outputs: (Linear)\n",
    "        Output is a 2-Dimensional Tensor. Each dimensions indicating\n",
    "        (batch_size, latent_dim)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, p = 2, axis = 1, epsilon: float = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if p == 1:\n",
    "            self.norm = lambda x: x * tf.math.rsqrt(tf.reduce_sum(tf.abs(x), keepdims = True, axis = axis) + epsilon)\n",
    "        elif p == 2:\n",
    "            self.norm = lambda x: x * tf.math.rsqrt(tf.reduce_sum(tf.square(x), keepdims = True, axis = axis) + epsilon)\n",
    "        else:\n",
    "            raise ValueError('`p` value should be 1 or 2.\\n\\t1 indicates L1 Norm.\\n\\t2 indicates L2 Norm.')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.norm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5512e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMapping(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Linear Mapping\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Define the latent dimension. | default -> 512\n",
    "        \n",
    "    Inputs: (Linear)\n",
    "        Input is a 2-Dimensional Tensor. Each dimensions indicating\n",
    "        (batch_size, latent_dim)\n",
    "        \n",
    "    Outputs: (Linear)\n",
    "        Output is a 2-Dimensional Tensor. Each dimensions indicating\n",
    "        (batch_size, latent_dim)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, latent_dim: int = 512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm = Normalize(p = 2, axis = 1)\n",
    "        \n",
    "        self.linear_1 = Linear(neurons = latent_dim)\n",
    "        self.act_1 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_2 = Linear(neurons = latent_dim)\n",
    "        self.act_2 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_3 = Linear(neurons = latent_dim)\n",
    "        self.act_3 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_4 = Linear(neurons = latent_dim)\n",
    "        self.act_4 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_5 = Linear(neurons = latent_dim)\n",
    "        self.act_5 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_6 = Linear(neurons = latent_dim)\n",
    "        self.act_6 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_7 = Linear(neurons = latent_dim)\n",
    "        self.act_7 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.linear_8 = Linear(neurons = latent_dim)\n",
    "        self.act_8 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        \n",
    "        x = self.act_1(self.linear_1(x))\n",
    "        x = self.act_2(self.linear_2(x))\n",
    "        x = self.act_3(self.linear_3(x))\n",
    "        x = self.act_4(self.linear_4(x))\n",
    "        x = self.act_5(self.linear_5(x))\n",
    "        x = self.act_6(self.linear_6(x))\n",
    "        x = self.act_7(self.linear_7(x))\n",
    "        x = self.act_8(self.linear_8(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3f8fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.models.Model):\n",
    "    def __init__(self, latent_dim: int = 512, gen_res: int = 1024, d_steps: int = 1, drift_weight: float = 0.001, \n",
    "                 seperate_latent: bool = True, gp_weight: float = 10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_res = gen_res\n",
    "        self.num_block = int(np.log2(gen_res) - 1)\n",
    "        self.seperate_latent = seperate_latent\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_steps = d_steps\n",
    "        self.drift_weight = drift_weight\n",
    "        self.FILTERS = [512, 512, 512, 512, 256, 128, 64, 32, 16]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return\n",
    "        \n",
    "    def __init_generator(self):\n",
    "        \n",
    "        if self.seperate_latent:\n",
    "            latent_inp = [\n",
    "                tf.keras.layers.Input(shape = (self.latent_dim), dtype = tf.float32, name = f'latent_inp_{i}')\n",
    "                for i in range(self.num_block)\n",
    "            ]\n",
    "        else:\n",
    "            latent_inp = tf.keras.layers.Input(shape = (self.latent_dim), dtype = tf.float32, name = 'latent_inp')\n",
    "            \n",
    "        const_inp = tf.keras.layers.Input(shape = (4, 4, 512), dtype = tf.float32, name = 'constant_input_4x4x512')\n",
    "        \n",
    "        noise_inp = [\n",
    "            tf.keras.layers.Input(shape = (2**(i+2), 2**(i+2), 1), dtype = tf.float32, \n",
    "                                  name = f'noise_input_{2**(i+2)}x{2**(i+2)}') for i in range(self.num_block)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        linear_mapping = LinearMapping(self.latent_dim)\n",
    "        if self.seperate_latent:\n",
    "            mapped = []\n",
    "            for lp in latent_inp:\n",
    "                mapped.append(linear_mapping(lp))\n",
    "        else:\n",
    "            mapped = linear_mapping(latent_inp)\n",
    "            \n",
    "            \n",
    "        if self.gen_type == 'skip':\n",
    "            \n",
    "            if self.seperate_latent:\n",
    "                x = StyleBlock(filters = self.FILTERS[0])([const_inp, mapped[0], noise_inp[0]])\n",
    "                rgb = ToRGB()([x, mapped[0]])\n",
    "                \n",
    "                for i in range(1, self.num_block):\n",
    "                    x, rgb = SkipGeneratorBlock(filters = self.FILTERS[i])([x, mapped[i], noise_inp[i], rgb])\n",
    "                    \n",
    "                return tf.keras.models.Model(latent_inp + [const_inp] + noise_inp, rgb, \n",
    "                                             name = f'skip_generator_{self.gen_res}x{self.gen_res}')\n",
    "            \n",
    "            else:\n",
    "                x = StyleBlock(filters = self.FILTERS[0])([const_inp, mapped, noise_inp[0]])\n",
    "                rgb = ToRGB()([x, mapped])\n",
    "                \n",
    "                for i in range(1, self.num_block):\n",
    "                    x, rgb = SkipGeneratorBlock(filters = self.FILTERS[i])([x, mapped, noise_inp[i], rgb])\n",
    "                    \n",
    "                return tf.keras.models.Model([latent_inp, const_inp] + noise_inp, rgb, \n",
    "                                             name = f'skip_generator_{self.gen_res}x{self.gen_res}')\n",
    "            \n",
    "        elif self.gen_type == 'residual':\n",
    "            \n",
    "            if self.seperate_latent:\n",
    "                x = StyleBlock(filters = self.FILTERS[0])([const_inp, mapped[0], noise_inp[0]])\n",
    "                \n",
    "                for i in range(1, self.num_block):\n",
    "                    x = ResidualGeneratorBlock(filters = self.FILTERS[i])([x, mapped[i], noise_inp[i]])\n",
    "                \n",
    "                out = ToRGB()([x, mapped[i]])\n",
    "                return tf.keras.models.Model(latent_inp + [const_inp] + noise_inp, out, \n",
    "                                             name = f'residual_generator_{self.gen_res}x{self.gen_res}')\n",
    "            \n",
    "            else:\n",
    "                x = StyleBlock(filters = self.FILTERS[0])([const_inp, mapped, noise_inp[0]])\n",
    "                \n",
    "                for i in range(1, self.num_block):\n",
    "                    x = ResidualGeneratorBlock(filters = self.FILTERS[i])([x, mapped, noise_inp[i]])\n",
    "                \n",
    "                out = ToRGB()([x, mapped])\n",
    "                return tf.keras.models.Model([latent_inp, const_inp] + noise_inp, out, \n",
    "                                             name = f'residual_generator_{self.gen_res}x{self.gen_res}')\n",
    "            \n",
    "    def __init_discriminator(self):\n",
    "        \n",
    "        inp = tf.keras.layers.Input(shape = (self.gen_res, self.gen_res, 3), dtype = tf.float32, \n",
    "                                    name = f'discriminator_input_{self.gen_res}x{self.gen_res}x3')\n",
    "        \n",
    "        \n",
    "        if self.disc_type == 'residual':\n",
    "            x = FromRGB(filters = self.FILTERS[-1])(inp)\n",
    "            \n",
    "            for i in range(self.num_block-1, 0, -1):\n",
    "                x = ResidualDiscriminatorBlock(filters = self.FILTERS[i])(x)\n",
    "                \n",
    "            x = MinibatchStddev()(x)\n",
    "            x = Conv2D(filters = self.FILTERS[0], kernel_size = (3, 3), strides = (2, 2))(x)\n",
    "            x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "            \n",
    "            x = tf.keras.layers.Flatten()(x)\n",
    "            x = Linear(neurons = 1, gain = 1.0)(x)\n",
    "            return tf.keras.models.Model(inp, x, name = f'residual_discriminator_{self.gen_res}x{self.gen_res}')\n",
    "        \n",
    "        elif self.disc_type == 'skip':\n",
    "            out_1, out_2 = SkipDiscriminatorBlock(filters = self.FILTERS[-2], first_block = True)([inp])\n",
    "            for i in range(self.num_block-2, 0, -1):\n",
    "                out_1, out_2 = SkipDiscriminatorBlock(filters = self.FILTERS[i])([out_1, out_2])\n",
    "                \n",
    "            out = MinibatchStddev()(out_2)\n",
    "            out = Conv2D(filters = self.FILTERS[0], kernel_size = (3, 3), strides = (2, 2))(out)\n",
    "            out = tf.keras.layers.LeakyReLU(alpha = 0.2)(out)\n",
    "            \n",
    "            out = tf.keras.layers.Flatten()(out)\n",
    "            out = Linear(neurons = 1, gain = 1.0)(out)\n",
    "            \n",
    "            return tf.keras.models.Model(inp, out, name = f'skip_discriminator_{self.gen_res}x{self.gen_res}')\n",
    "            \n",
    "    def compile(self, optimizer, gen_type: str = 'skip', disc_type: str = 'residual'):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.gen_type = gen_type.lower()\n",
    "        self.disc_type = disc_type.lower()\n",
    "        \n",
    "        self.generator = self.__init_generator()\n",
    "        self.discriminator = self.__init_discriminator()\n",
    "        \n",
    "        self.step = 0\n",
    "        \n",
    "    def generator_loss(self, disc_gen_out):\n",
    "        return -tf.reduce_mean(disc_gen_out)\n",
    "    \n",
    "    def discriminator_loss(self, disc_real_out, disc_gen_out):\n",
    "        return tf.reduce_mean(disc_gen_out) - tf.reduce_mean(disc_real_out)\n",
    "    \n",
    "    def gradient_penalty(self, real_img, gen_img):\n",
    "        \n",
    "        batch_size = tf.shape(real_img)[0]\n",
    "        epsilon = tf.random.uniform((batch_size, 1, 1, 1), minval = 0.0, maxval = 1.0)\n",
    "        interpolated_img = ((1 - epsilon) * real_img) + (epsilon * gen_img)\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_img)\n",
    "            out = self.discriminator(interpolated_img)\n",
    "        \n",
    "        grads = gp_tape.gradient(out, [interpolated_img])[0]\n",
    "        norm = tf.square(tf.reduce_mean(tf.square(grads), keepdims = True, axis = [1, 2, 3]))\n",
    "        gp = tf.reduce_mean(tf.square(norm - 1)) * self.gp_weight\n",
    "        return gp\n",
    "    \n",
    "    def drift_loss(self, disc_real_out):\n",
    "        return tf.reduce_mean(tf.square(disc_real_out)) * self.drift_weight\n",
    "    \n",
    "\n",
    "#     def path_length_penalty(self, gen_out, w, beta = 0.995):\n",
    "#         y = tf.random.normal(tf.shape(gen_out))\n",
    "        \n",
    "#         img_size = tf.cast(gen_out.shape[1] * gen_out.shape[2], tf.float32)\n",
    "#         out = tf.math.reduce_sum(gen_out * y) * tf.math.rsqrt(img_size)\n",
    "        \n",
    "#         grads = tf.gradients(out, w)\n",
    "#         norm = tf.sqrt(tf.math.reduce_mean(tf.math.reduce_sum(tf.square(grads), axis = 2), axis = 1))\n",
    "        \n",
    "#         pl_mean = tf.reduce_mean(norm)\n",
    "        \n",
    "        \n",
    "#         return tf.square(out - pl_mean)\n",
    "        \n",
    "    \n",
    "    def generate_noise(self, batch_size):\n",
    "        return [tf.random.normal((batch_size, 2**(i+2), 2**(i+2), 1)) for i in range(self.num_block)]\n",
    "    \n",
    "    def generate_latent(self, batch_size):\n",
    "            return [tf.random.normal((batch_size, self.latent_dim)) \n",
    "                    for _ in range(self.num_block)] if self.seperate_latent else tf.random.normal((batch_size, \n",
    "                                                                                                         self.latent_dim))\n",
    "\n",
    "        \n",
    "    def train_step(self, real_img):\n",
    "        if isinstance(real_img, tuple):\n",
    "            real_img = real_img[0]\n",
    "        batch_size = tf.shape(real_img)[0]\n",
    "            \n",
    "        for _ in range(self.d_steps):\n",
    "            \n",
    "            const_inp = tf.ones((batch_size, 4, 4, 512))\n",
    "            latent_inp = self.generate_latent(batch_size)\n",
    "            noise_inp = self.generate_noise(batch_size)\n",
    "            gen_inp = latent_inp + [const_inp] + noise_inp if self.seperate_latent else [latent_inp, const_inp] + noise_inp\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                gen_out = self.generator(gen_inp, training = True)\n",
    "                \n",
    "                disc_real_out = self.discriminator(real_img, training = True)\n",
    "                disc_gen_out = self.discriminator(gen_out, training = True)\n",
    "                \n",
    "                gp_loss = self.gradient_penalty(real_img, gen_out)\n",
    "                drf_loss = self.drift_loss(disc_real_out)\n",
    "                _disc_loss = self.discriminator_loss(disc_real_out, disc_gen_out)\n",
    "                # pl = self.path_length_penalty(gen_out, latent_inp)\n",
    "                disc_loss = _disc_loss + gp_loss + drf_loss # + pl\n",
    "                \n",
    "            disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "            \n",
    "        \n",
    "        const_inp = tf.ones((batch_size, 4, 4, 512))\n",
    "        latent_inp = self.generate_latent(batch_size)\n",
    "        noise_inp = self.generate_noise(batch_size)\n",
    "        gen_inp = latent_inp + [const_inp] + noise_inp if self.seperate_latent else [latent_inp, const_inp] + noise_inp\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_out = self.generator(gen_inp, training = True)\n",
    "            disc_gen_out = self.discriminator(gen_out, training = True)\n",
    "            gen_loss = self.generator_loss(disc_gen_out)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        return {'disc_loss': disc_loss, 'gp_loss': gp_loss, 'drf_loss': drf_loss, '_disc_loss': _disc_loss, 'gen_loss': gen_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f853bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path = r'E:\\Image Datasets\\Celeb A\\Dataset\\img_align_celeba'\n",
    "image_size = 256\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 1000\n",
    "\n",
    "train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function = lambda x: tf.cast((x/127.5)-1, tf.float32))\n",
    "\n",
    "train_data = train_data_generator.flow_from_directory(directory  = path, target_size = (image_size, image_size),\n",
    "                                                 shuffle = True, batch_size = BATCH_SIZE, class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edbfbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(seperate_latent = True, gen_res = image_size)\n",
    "gan.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    gen_type = 'skip', disc_type = 'residual'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a03a654d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  45/1000 [>.............................] - ETA: 3:36 - disc_loss: 10.0000 - gp_loss: 10.0000 - drf_loss: 3.2238e-11 - _disc_loss: -1.4675e-07 - gen_loss: 2.1068e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17652/1291649556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.fit(train_data, epochs = EPOCHS, steps_per_epoch = STEPS_PER_EPOCH, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a336ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829d2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow((((gan.generator([tf.random.normal((1, 512)) for _ in range(7)] + [tf.ones((1, 4, 4, 512))] + \n",
    "#               [tf.random.normal((1, 4, 4, 1)), tf.random.normal((1, 8, 8, 1)), tf.random.normal((1, 16, 16, 1)), \n",
    "#                tf.random.normal((1, 32, 32, 1)), tf.random.normal((1, 64, 64, 1)), tf.random.normal((1, 128, 128, 1)), \n",
    "#                tf.random.normal((1, 256, 256, 1))])[0])+1)*127.5).numpy().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan = GAN(seperate_latent = True)\n",
    "# gan.compile(optimizer=None, gen_type = 'skip')\n",
    "# gan.compile(optimizer=None, gen_type = 'residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b0705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gan.generator().summary()\n",
    "# gan.discriminator().summary(line_length = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec1fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(gan.generator(), show_shapes = True, dpi = 64)\n",
    "# tf.keras.utils.plot_model(gan.discriminator(), show_shapes = True, dpi = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd091cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(gan.generator(), show_shapes = True, dpi = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
