{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527b21f0",
   "metadata": {},
   "source": [
    "# MSG-Gan (Multi Scale Gradient Gan)\n",
    "\n",
    "This is an attempt to re-implement the paper MSG-GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1903.06048.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://github.com/akanimax/msg-gan-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407596dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_img = 1024\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa904808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(file):\n",
    "    images = tf.io.decode_png(tf.io.read_file(file), channels = 3)\n",
    "    \n",
    "    images = tf.image.resize(images, [res_img, res_img], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    \n",
    "    images = (images/127.5)-1\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1569c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, num_examples = 1000):\n",
    "    data = glob(path)[:num_examples]\n",
    "    return tf.data.Dataset.list_files(data).map(load_files).shuffle(num_examples).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('E://Image Datasets//Celeb A//Dataset//img_align_celeba//img_align_celeba//*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f32226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767836a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStddev(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inp_shape = tf.shape(inputs)\n",
    "        \n",
    "        mean = tf.math.reduce_mean(inputs, axis = 0, keepdims = True)\n",
    "        std = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(inputs - mean), axis = 0, keepdims = True) + self.epsilon)\n",
    "        avg_std = tf.math.reduce_mean(std, keepdims = True)\n",
    "        tiled = tf.tile(avg_std, (inp_shape[0], inp_shape[1], inp_shape[2], 1))\n",
    "        combined = tf.concat([inputs, tiled], axis = -1)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22651a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRGB(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = 3, kernel_size = (1, 1), strides = (1, 1), padding = 'same')\n",
    "        self.act = tf.keras.layers.Activation('tanh')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.act(self.conv(inputs))\n",
    "    \n",
    "    \n",
    "class FromRGB(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1), padding = 'same')\n",
    "        self.act = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.act(self.conv(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3931a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, sampling = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sampling = sampling\n",
    "        if sampling is not None:\n",
    "            if sampling == 'up':\n",
    "                self.up_sample = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'bilinear')\n",
    "            elif sampling == 'down':\n",
    "                self.down_sample = tf.keras.layers.AveragePooling2D()\n",
    "            \n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), padding = 'same')\n",
    "        self.act_1 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), padding = 'same')\n",
    "        self.act_2 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "            \n",
    "    def call(self, x):\n",
    "        if self.sampling == 'up':\n",
    "            x = self.up_sample(x)\n",
    "        x = self.act_1(self.conv_1(x))\n",
    "        x = self.act_2(self.conv_2(x))\n",
    "        if self.sampling == 'down':\n",
    "            x = self.down_sample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_dim = 512, filters = None):\n",
    "    inp = tf.keras.layers.Input(shape = (1, 1, latent_dim), dtype = tf.float32, name = 'generator_latent_input')\n",
    "    rgb_outs = []\n",
    "    filters = [512, 512, 512, 512, 256, 128, 64, 32, 16] if filters is None else filters\n",
    "\n",
    "    g = tf.keras.layers.Conv2DTranspose(filters = filters[0], kernel_size = (4, 4))(inp)\n",
    "    g = tf.keras.layers.LeakyReLU(alpha = 0.2)(g)\n",
    "    g = tf.keras.layers.Conv2D(filters = latent_dim, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(g)\n",
    "    g = tf.keras.layers.LeakyReLU(alpha = 0.2)(g)\n",
    "    rgb_outs.append(ToRGB()(g))\n",
    "    \n",
    "    for f in filters[1:]:\n",
    "        g = GenBlock(f, sampling = 'up')(g)\n",
    "        rgb_outs.append(ToRGB()(g))\n",
    "\n",
    "    return tf.keras.models.Model(inp,  rgb_outs)\n",
    "\n",
    "def discriminator(n_inps = 9, combine_function = 'simple', logits = True):# np.log2(1024) - 1\n",
    "    inputs = []\n",
    "    for i in range(n_inps):\n",
    "        res = 2**(i+2)\n",
    "        inputs.append(tf.keras.layers.Input(shape = (res, res, 3), dtype = tf.float32, \n",
    "                                            name = f'discriminator_input_{res}x{res}x3'))\n",
    "        \n",
    "    filters = [512, 512, 512, 512, 256, 128, 64, 32, 16]\n",
    "\n",
    "    x = inputs[len(inputs) - 1]\n",
    "    for i in range(len(inputs) - 1, 0, -1):\n",
    "        x = FromRGB(filters[i])(x)\n",
    "        x = GenBlock(filters = filters[i], sampling = 'down')(x)\n",
    "        \n",
    "        if combine_function == 'simple':\n",
    "            x = tf.keras.layers.Concatenate()([inputs[i - 1], x])\n",
    "        elif combine_function == 'lin_cat':\n",
    "            x = tf.keras.layers.Concatenate()([FromRGB(filters[i])(inputs[i - 1]), x])\n",
    "        elif combine_function == 'cat_lin':\n",
    "            x = FromRGB(filters[i])(tf.keras.layers.Concatenate()([inputs[i - 1], x]))\n",
    "            \n",
    "    x = MinibatchStddev()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters = filters[i], kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters = filters[i], kernel_size = (4, 4), strides = (1, 1), padding = 'valid')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 1, kernel_size = (1, 1), strides = (1, 1), padding = 'same')(x)\n",
    "    \n",
    "    if logits:\n",
    "        x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "        \n",
    "    return tf.keras.models.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses(object):\n",
    "    def __init__(self, loss_types = ['wgan_gp', 'adversarial']):\n",
    "        if isinstance(loss_types, list) | isinstance(loss_types, tuple):\n",
    "            self.loss_types = loss_types\n",
    "        elif isinstance(loss_types, str):\n",
    "            self.loss_types = [loss_types]\n",
    "        else:\n",
    "            raise Exception('Invalid Losses.')\n",
    "        \n",
    "    def gradient_penalty(self, disc, real, gen, lambda_ = 10.0):\n",
    "        \n",
    "        interpolated = []\n",
    "        for r, g in zip(real, gen):\n",
    "            epsilon = tf.random.uniform((r.shape[0], 1, 1, 1), 0.0, 1.0)\n",
    "            interpolated.append(((1.0 - epsilon) * r) + (epsilon * g))\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            out = disc(interpolated)\n",
    "        grads = gp_tape.gradient(out, [interpolated])[0]\n",
    "        \n",
    "        gp = 0\n",
    "        for g in grads:\n",
    "            norm = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(g), axis = [1, 2, 3], keepdims = True) + 1e-8)\n",
    "            gp += tf.math.reduce_mean(tf.square(norm - 1.0)) * lambda_\n",
    "            \n",
    "        return gp\n",
    "        \n",
    "    def discriminator_loss(self, disc_real_out, disc_gen_out):\n",
    "        loss = 0\n",
    "        for loss_type in self.loss_types:\n",
    "            if loss_type.__contains__('wgan'):\n",
    "                loss += tf.math.reduce_mean(disc_gen_out) - tf.math.reduce_mean(disc_real_out)\n",
    "\n",
    "            elif loss_type == 'lsgan':\n",
    "                loss += tf.math.reduce_mean(tf.math.square(tf.ones_like(disc_real_out) - disc_real_out)) \n",
    "                loss += tf.math.reduce_mean(tf.math.square(tf.zeros_like(disc_gen_out) - disc_gen_out))\n",
    "\n",
    "            elif loss_type == 'adversarial':\n",
    "                loss -= tf.math.reduce_mean(tf.math.log(disc_real_out)) + tf.math.reduce_mean(tf.math.log(1.0 - disc_gen_out))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def generator_loss(self, disc_gen_out):\n",
    "        loss = 0\n",
    "        for loss_type in self.loss_types:\n",
    "            if loss_type.__contains__('wgan'):\n",
    "                loss -= tf.math.reduce_mean(disc_gen_out)\n",
    "\n",
    "            elif loss_type == 'lsgan':\n",
    "                loss += tf.math.reduce_mean(tf.math.square(tf.ones_like(disc_gen_out) - disc_gen_out))\n",
    "\n",
    "            elif loss_type == 'adversarial':\n",
    "                loss -= tf.math.reduce_mean(tf.math.log(disc_gen_out))\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, latent_dim = 512, learning_rate = 0.003, loss_types = ['wgan_gp', 'adversarial']):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.loss_types = loss_types\n",
    "        self.gen_opt = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
    "        self.disc_opt = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
    "        \n",
    "        self.generator = generator(latent_dim = latent_dim)\n",
    "        self.discriminator = discriminator()\n",
    "        \n",
    "        self.losses = Losses(loss_types = loss_types)\n",
    "        \n",
    "    def generate_diff_dim_imgs(self, img, till = 4):\n",
    "        assert img.shape[1] == img.shape[2]\n",
    "        outs = [img]\n",
    "        while outs[-1].shape[1] != 4:\n",
    "            outs.append(tf.keras.layers.AveragePooling2D()(outs[-1]))\n",
    "        return outs\n",
    "    \n",
    "    def generate_latent_noise(self, batch_size):\n",
    "        return tf.random.normal((batch_size, 1, 1, self.latent_dim))\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, img):\n",
    "        \n",
    "        real_imgs = self.generate_diff_dim_imgs(img)[::-1]\n",
    "        latent_inp = self.generate_latent_noise(img.shape[0])\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_outs = self.generator(latent_inp, training = True)\n",
    "            \n",
    "            assert len(real_imgs) == len(gen_outs), f'{len(real_imgs)}, {len(gen_outs)}'\n",
    "            disc_real_outs = self.discriminator(real_imgs, training = True)\n",
    "            disc_gen_outs = self.discriminator(gen_outs, training = True)\n",
    "            \n",
    "            gen_loss = self.losses.generator_loss(disc_gen_outs)\n",
    "            disc_loss = self.losses.discriminator_loss(disc_real_outs, disc_gen_outs)\n",
    "            \n",
    "            for loss_type in self.loss_types:\n",
    "                if loss_type.__contains__('gp'): \n",
    "                    disc_loss += self.losses.gradient_penalty(self.discriminator, real_imgs, gen_outs)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return gen_loss, disc_loss\n",
    "    \n",
    "    def train(self, data, epochs = 1):\n",
    "        gen_losses, disc_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print(f'Epoch: {e} Starts')\n",
    "            for img in data:\n",
    "                gen_loss, disc_loss = self.train_step(img)\n",
    "                print('.', end = '')\n",
    "                \n",
    "            gen_losses.append(gen_loss)\n",
    "            disc_losses.append(disc_loss)\n",
    "            print(f'\\nGenerator Loss: {gen_loss} \\t Discriminator Loss: {disc_loss}')\n",
    "            print(f'Epoch: {e} Ends\\n')\n",
    "            \n",
    "        return {'gen_losses': gen_losses, 'disc_losses': disc_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449067af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = trainer.train(dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37877b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
