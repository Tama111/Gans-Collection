{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31d88e8",
   "metadata": {},
   "source": [
    "# SA-Gan (Self - Attention Gan)\n",
    "\n",
    "This is an attempt to re-implement the paper SA-GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1805.08318.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://github.com/taki0112/Self-Attention-GAN-Tensorflow\n",
    "* https://github.com/brain-research/self-attention-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82797442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import def_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E://Image Datasets//Celeb A\\Dataset//img_align_celeba//img_align_celeba'\n",
    "num_examples = 1000\n",
    "batch_size = 64\n",
    "img_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(file):\n",
    "    images = tf.io.decode_png(tf.io.read_file(file), channels = 3)\n",
    "    \n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images =  tf.image.resize(images, [img_size, img_size], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    images = (images/127.5) - 1\n",
    "    \n",
    "    return images\n",
    "\n",
    "def load_data(path):\n",
    "    data = glob(path + '//*.jpg')[:num_examples]\n",
    "    return tf.data.Dataset.list_files(data).map(load_files).shuffle(num_examples).batch(batch_size)\n",
    "\n",
    "dataset = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54134b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced: spectral norm\n",
    "# https://gist.github.com/FloydHsiu/828eea345e1ca6950e05bb42f0a75b50\n",
    "# https://gist.github.com/FloydHsiu/ab33c7d98d78f9873757810e2f8db50d\n",
    "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            \n",
    "        if not hasattr(self.layer, 'weight'):\n",
    "            self.w = tf.convert_to_tensor(self.layer.get_weights()[0])\n",
    "        else:\n",
    "            # self.w should not be trainable\n",
    "            self.w = tf.identity(self.layer.weight)\n",
    "        \n",
    "        if not hasattr(self, 'w'):\n",
    "            raise ValueError()\n",
    "        \n",
    "        self.w_shape = self.w.shape.as_list()\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "        self.u = self.add_weight(shape = (1, self.w_shape[-1]), initializer = init, \n",
    "                                   trainable = False, name = 'specrtal_norm_u')\n",
    "        \n",
    "        # super().build()\n",
    "        \n",
    "    @def_function.function\n",
    "    def call(self, inputs, training = None):\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "            \n",
    "        if training == True:\n",
    "            self._compute_weights()\n",
    "            \n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "    \n",
    "    def _compute_weights(self):\n",
    "        w_reshaped = tf.reshape(self.w, (-1, self.w_shape[-1]))\n",
    "        eps = 1e-12\n",
    "        \n",
    "        _u = tf.identity(self.u)\n",
    "        _v = tf.matmul(_u, tf.transpose(w_reshaped, perm = [1, 0]))\n",
    "        _v /= tf.maximum(tf.math.reduce_sum(_v ** 2) ** 0.5, eps)\n",
    "        _u = tf.matmul(_v, w_reshaped)\n",
    "        _u /= tf.maximum(tf.math.reduce_sum(_u ** 2) ** 0.5, eps)\n",
    "        \n",
    "        _u = tf.stop_gradient(_u)\n",
    "        _v = tf.stop_gradient(_v)\n",
    "        \n",
    "        self.u.assign(_u)\n",
    "        sigma = tf.matmul(tf.matmul(_v, w_reshaped), tf.transpose(_u, perm = [1, 0]))\n",
    "        \n",
    "        self.layer.weight.assign(self.w / sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        inp_neurons = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "        self.weight = self.add_weight(shape = (inp_neurons, self.neurons), initializer = init, \n",
    "                                 trainable = True, name = 'weight')\n",
    "        self.bias = self.add_weight(shape = (1, self.neurons), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'bias')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.matmul(inputs, self.weight), self.bias)\n",
    "    \n",
    "class SNLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear_sn = SpectralNormalization(Linear(neurons = neurons))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.linear_sn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = 'SAME' if (kernel_size[0] - 1)//2 else 'VALID'\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        inp_filters = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "        self.weight = self.add_weight(shape = self.kernel_size + (inp_filters, self.filters), initializer = init, \n",
    "                                      trainable = True, name = 'kernel')\n",
    "        self.bias = self.add_weight(shape = (1, 1, 1, self.filters), initializer = 'zeros', \n",
    "                                    trainable = True, name = 'bias')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.nn.conv2d(inputs, self.weight, self.strides, self.padding), self.bias)\n",
    "    \n",
    "    \n",
    "class SNConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_sn = SpectralNormalization(Conv2D(filters = filters, kernel_size = kernel_size, strides = strides))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.conv_sn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, k = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.inp_shp = input_shape\n",
    "        \n",
    "        self.gamma = self.add_weight(shape = (1, ), initializer = 'zeros', \n",
    "                                     trainable = True, name = 'gamma')\n",
    "        \n",
    "        chn = self.inp_shp[-1]\n",
    "        self.f = SNConv2D(filters = chn//self.k, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.g = SNConv2D(filters = chn//self.k, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.h = SNConv2D(filters = chn, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.v = SNConv2D(filters = chn, kernel_size = (1, 1), strides = (1, 1))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fx = self.f(inputs)\n",
    "        gx = self.g(inputs)\n",
    "        hx = self.h(inputs)\n",
    "        \n",
    "        location_num = self.inp_shp[1] * self.inp_shp[2]\n",
    "        gx = tf.reshape(gx, (-1, location_num, gx.shape[-1]))\n",
    "        fx = tf.reshape(fx, (-1, location_num, fx.shape[-1]))\n",
    "        hx = tf.reshape(hx, (-1, location_num, hx.shape[-1]))\n",
    "        \n",
    "        s = tf.matmul(gx, fx, transpose_b = True)\n",
    "        beta = tf.nn.softmax(s, axis = -1)\n",
    "        o = tf.matmul(beta, hx)\n",
    "        \n",
    "        out = tf.reshape(o, (-1, self.inp_shp[1], self.inp_shp[2], self.inp_shp[3]))\n",
    "        out = self.v(out)\n",
    "        \n",
    "        out = self.gamma * out + inputs\n",
    "        return out\n",
    "\n",
    "class NonLocalBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, k = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.inp_shp = input_shape\n",
    "        \n",
    "        self.sigma = self.add_weight(shape = (1, ), initializer = 'zeros', \n",
    "                                     trainable = True, name = 'sigma')\n",
    "        \n",
    "        chn = self.inp_shp[-1]\n",
    "        self.f = SNConv2D(filters = chn//self.k, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.g = SNConv2D(filters = chn//self.k, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.h = SNConv2D(filters = chn//2, kernel_size = (1, 1), strides = (1, 1))\n",
    "        self.v = SNConv2D(filters = chn, kernel_size = (1, 1), strides = (1, 1))\n",
    "        \n",
    "        self.max_pool_1 = tf.keras.layers.MaxPool2D()\n",
    "        self.max_pool_2 = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        location_num = self.inp_shp[1] * self.inp_shp[2]\n",
    "        downsampled_num = location_num // 4\n",
    "        \n",
    "        fx = self.f(inputs)\n",
    "        fx = tf.reshape(fx, (-1, location_num, fx.shape[-1]))\n",
    "        \n",
    "        gx = self.max_pool_1(self.g(inputs))\n",
    "        gx = tf.reshape(gx, (-1, downsampled_num, gx.shape[-1]))\n",
    "        \n",
    "        attn = tf.nn.softmax(tf.matmul(fx, gx, transpose_b = True), axis = -1)\n",
    "        \n",
    "        hx = self.max_pool_2(self.h(inputs))\n",
    "        hx = tf.reshape(hx, (-1, downsampled_num, hx.shape[-1]))\n",
    "        \n",
    "        out = tf.matmul(attn, hx)\n",
    "        out = tf.reshape(out, (-1, self.inp_shp[1], self.inp_shp[2], self.inp_shp[-1]//2))\n",
    "        out = self.v(out)\n",
    "        \n",
    "        return self.sigma * out + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef03fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGAN(object):\n",
    "    def __init__(self, img_res = 128, latent_dim = 1042):\n",
    "        self.img_res = img_res\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def up_sample_res_block(self, inp, filters):\n",
    "        x = tf.keras.layers.BatchNormalization()(inp)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'nearest')(x)\n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        \n",
    "        inp = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'nearest')(inp)\n",
    "        inp = SNConv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1))(inp)\n",
    "        \n",
    "        return inp + x\n",
    "        \n",
    "    def down_sample_res_block(self, inp, filters, down_sample = True):\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(inp)\n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        \n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        \n",
    "        if down_sample:\n",
    "            x = tf.keras.layers.AveragePooling2D()(x)\n",
    "        \n",
    "        if down_sample or (inp.shape[-1] != filters):\n",
    "            inp = SNConv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1))(inp)\n",
    "            if down_sample:\n",
    "                inp = tf.keras.layers.AveragePooling2D()(inp)\n",
    "        \n",
    "        return x + inp\n",
    "    \n",
    "    def optimized_block(self, inp, filters):\n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(inp)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = SNConv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        x = tf.keras.layers.AveragePooling2D()(x)\n",
    "        \n",
    "        inp = tf.keras.layers.AveragePooling2D()(inp)\n",
    "        inp = SNConv2D(filters = filters, kernel_size = (1, 1), strides = (1, 1))(inp)\n",
    "        \n",
    "        return x + inp\n",
    "        \n",
    "        \n",
    "    def generator(self):\n",
    "        inp = tf.keras.layers.Input(shape = (self.latent_dim, ), dtype = tf.float32, name = 'generator_input')\n",
    "        \n",
    "        x = SNLinear(neurons = 4*4*1024)(inp)\n",
    "        x = tf.keras.layers.Reshape((4, 4, 1024))(x)\n",
    "        \n",
    "        x = self.up_sample_res_block(inp = x, filters = 1024)\n",
    "        x = self.up_sample_res_block(inp = x, filters = 512)\n",
    "        x = self.up_sample_res_block(inp = x, filters = 256)\n",
    "        \n",
    "        ################################################\n",
    "        # attention layer\n",
    "        ## both the ways are similar\n",
    "        x = NonLocalBlock()(x)\n",
    "        # x = SelfAttention()(x)\n",
    "        ################################################\n",
    "        \n",
    "        x = self.up_sample_res_block(inp = x, filters = 128)\n",
    "        x = self.up_sample_res_block(inp = x, filters = 64)\n",
    "        \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = SNConv2D(filters = 3, kernel_size = (3, 3), strides = (1, 1))(x)\n",
    "        x = tf.keras.layers.Activation('tanh')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, x, name = 'Generator')\n",
    "    \n",
    "    def discriminator(self):\n",
    "        inp = tf.keras.layers.Input(shape = (self.img_res, self.img_res, 3), dtype = tf.float32, \n",
    "                                    name = 'discriminator_input')\n",
    "        \n",
    "        x = self.optimized_block(inp = inp, filters = 64)\n",
    "        x = self.down_sample_res_block(inp = x, filters = 128)\n",
    "        \n",
    "        ################################################\n",
    "        # attention layer\n",
    "        # both the ways are similar\n",
    "        x = NonLocalBlock()(x)\n",
    "        # x = SelfAttention()(x)\n",
    "        ################################################\n",
    "        \n",
    "        x = self.down_sample_res_block(inp = x, filters = 256)\n",
    "        x = self.down_sample_res_block(inp = x, filters = 512)\n",
    "        x = self.down_sample_res_block(inp = x, filters = 1024)\n",
    "        x = self.down_sample_res_block(inp = x, filters = 1024, down_sample = False)\n",
    "        \n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = tf.math.reduce_mean(x, axis = [1, 2])\n",
    "        x = SNLinear(neurons = 1)(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, x, name = 'Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc91c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463a38c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):\n",
    "    def discriminator_loss(self, disc_real_out, disc_gen_out):\n",
    "        real_loss = -tf.math.reduce_mean(tf.minimum(0.0, disc_real_out - 1.0))\n",
    "        gen_loss = -tf.math.reduce_mean(tf.minimum(0.0, -1.0 - disc_gen_out))\n",
    "        return real_loss + gen_loss\n",
    "    \n",
    "    def generator_loss(self, disc_gen_out):\n",
    "        return -tf.math.reduce_mean(disc_gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7407a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, img_res = 128, latent_dim = 1024, gen_lr = 1e-4, disc_lr = 4e-4):\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.gen_opt = tf.keras.optimizers.Adam(learning_rate = gen_lr, beta_1 = 0.0, beta_2 = 0.9)\n",
    "        self.disc_opt = tf.keras.optimizers.Adam(learning_rate = disc_lr, beta_1 = 0.0, beta_2 = 0.9)\n",
    "        \n",
    "        gan = SAGAN(img_res = img_res, latent_dim = latent_dim)\n",
    "        self.generator = gan.generator()\n",
    "        self.discriminator = gan.discriminator()\n",
    "        \n",
    "        self.losses = Losses()\n",
    "        \n",
    "    def generate_latent_noise(self, batch_size):\n",
    "        return tf.random.normal((batch_size, self.latent_dim))\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, real_img):\n",
    "        latent_inp = self.generate_latent_noise(real_img.shape[0])\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_out = self.generator(latent_inp, training = True)\n",
    "            \n",
    "            disc_real_out = self.discriminator(real_img, training = True)\n",
    "            disc_gen_out = self.discriminator(gen_out, training = True)\n",
    "            \n",
    "            gen_loss = self.losses.generator_loss(disc_gen_out)\n",
    "            disc_loss = self.losses.discriminator_loss(disc_real_out, disc_gen_out)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return gen_loss, disc_loss\n",
    "    \n",
    "    def train(self, data, epochs = 1):\n",
    "        gen_losses, disc_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print(f'Epoch: {e} Starts')\n",
    "            for img in data:\n",
    "                gen_loss, disc_loss = self.train_step(img)\n",
    "                print('.', end = '')\n",
    "                \n",
    "            gen_losses.append(gen_loss)\n",
    "            disc_losses.append(disc_loss)\n",
    "            print(f'\\nGenerator Loss: {gen_loss} \\t Discriminator Loss: {disc_loss}')\n",
    "            print(f'Epoch: {e} Ends\\n')\n",
    "            \n",
    "        return {'gen_losses': gen_losses, 'disc_losses': disc_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ff655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_losses = trainer.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e252a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
