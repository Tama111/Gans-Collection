{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3140d6f3",
   "metadata": {},
   "source": [
    "# Star Gan\n",
    "\n",
    "This is an attempt to re-implement the paper Star GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1711.09020.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://github.com/yunjey/stargan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc54d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a548fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'img_shape': (128, 128, 3),\n",
    "    'num_classes': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502161c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon = 1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        inp_chn = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.gamma = self.add_weight(shape = (1, 1, 1, inp_chn), initializer = init, \n",
    "                                     trainable = True, name = 'gamma')\n",
    "        self.beta = self.add_weight(shape = (1, 1, 1, inp_chn), initializer = 'zeros', \n",
    "                                    trainable = True, name = 'beta')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean = tf.math.reduce_mean(inputs, axis = [1, 2], keepdims = True)\n",
    "        rstd = tf.math.rsqrt(tf.math.reduce_variance(inputs) + self.epsilon)\n",
    "        \n",
    "        out = self.gamma * ((inputs - mean) * rstd) + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58f6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, config):\n",
    "        self.img_shape = config['img_shape']\n",
    "        self.num_classes = config['num_classes']\n",
    "        \n",
    "    def __reshape_lbl(self, x):\n",
    "        x = tf.repeat(x, self.img_shape[0]*self.img_shape[1])\n",
    "        x = tf.reshape(x, (-1, self.img_shape[0], self.img_shape[1], self.num_classes))\n",
    "        return x\n",
    "    \n",
    "    def __residual_block(self, inp, filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same'):\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = padding)(inp)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = padding)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x, inp])\n",
    "        return x\n",
    "      \n",
    "    @property\n",
    "    def generator(self):\n",
    "        inp_img = tf.keras.layers.Input(shape = self.img_shape, dtype = tf.float32, \n",
    "                                        name = f'generator_img_input')\n",
    "        inp_lbl = tf.keras.layers.Input(shape = self.num_classes, dtype = tf.float32, \n",
    "                                        name = f'generator_label_input')\n",
    "        \n",
    "        lbl = tf.keras.layers.Lambda(self.__reshape_lbl)(inp_lbl)\n",
    "        x = tf.keras.layers.Concatenate(axis = -1)([inp_img, lbl])\n",
    "        \n",
    "        # Downsampling\n",
    "        x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (7, 7), strides = (1, 1), padding = 'same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 256, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        for _ in range(6):\n",
    "            x = self.__residual_block(x, filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same')\n",
    "            \n",
    "        \n",
    "        # UpSampling\n",
    "        x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 3, kernel_size = (7, 7), strides = (1, 1), padding = 'same')(x)\n",
    "        x = tf.keras.layers.Activation('tanh')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inputs = [inp_img, inp_lbl], outputs = x, name = f'Generator')\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def discriminator(self):\n",
    "        inp = tf.keras.layers.Input(shape = self.img_shape, dtype = tf.float32, name = f'discriminator')\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(inp)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 256, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 512, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 1024, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters = 2048, kernel_size = (4, 4), strides = (2, 2), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        D_src = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "        \n",
    "        D_cls = tf.keras.layers.Conv2D(filters = self.num_classes, \n",
    "                                       kernel_size = (self.img_shape[0]//64, self.img_shape[1]//64), \n",
    "                                       strides = (1, 1), padding = 'valid')(x)\n",
    "        D_cls = tf.keras.layers.Reshape((self.num_classes, ))(D_cls)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, [D_src, D_cls], name = 'Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac00dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan = GAN(config)\n",
    "discriminator = gan.discriminator\n",
    "generator = gan.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65f8499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):\n",
    "    def __init__(self, loss_type):\n",
    "        self.bce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "        if loss_type == 'sparse':\n",
    "            self.cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "        if loss_type == 'categorical':\n",
    "            self.cce = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "    \n",
    "    def gradeint_penalty(self, discriminator, real, gen, gp_weight = 10.0):\n",
    "        \n",
    "        epsilon = tf.random.uniform((real.shape[0], 1, 1, 1))\n",
    "        interpolated = ((1 - epsilon) * real) + (epsilon * gen)\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            out = discriminator(interpolated)\n",
    "        grads = gp_tape.gradient(out, [interpolated])[0]\n",
    "        norm = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(grads), axis = [1, 2, 3], keepdims = True))\n",
    "        gp = tf.math.reduce_mean(tf.square(norm - 1.0)) * gp_weight\n",
    "        return gp\n",
    "    \n",
    "    def disc_wgan_loss(self, disc_real_out_src, disc_gen_out_src):\n",
    "        return tf.math.reduce_mean(disc_gen_out_src) - tf.math.reduce_mean(disc_real_out_src)\n",
    "    \n",
    "    def gen_wgan_loss(self, disc_gen_out_src):\n",
    "        return -tf.math.reduce_mean(disc_gen_out_src)\n",
    "    \n",
    "    def adversarial_loss_disc(self, disc_real_out_src, disc_gen_out_src):\n",
    "        real_loss = self.bce(tf.ones_like(disc_real_out_src), disc_real_out_src)\n",
    "        gen_loss = self.bce(tf.zeros_like(disc_gen_out_src), disc_gen_out_src)\n",
    "        return real_loss + gen_loss\n",
    "    \n",
    "    def adversarial_loss_gen(self, disc_gen_out_src):\n",
    "        return self.bce(tf.ones_like(disc_gen_out_src), disc_gen_out_src)\n",
    "        \n",
    "    def reconstruction_loss(self, real, re_gen):\n",
    "        return tf.math.reduce_mean(tf.math.abs(real - re_gen))\n",
    "    \n",
    "    def cls_loss(self, tar_cls, disc_out_cls):\n",
    "        return self.cce(tar_cls, disc_out_cls)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfb4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, gan, config = None, n_disc = 5, learning_rate = 1e-4, \n",
    "                 lambda_cls = 1.0, lambda_rec = 10.0, loss_type = 'wgan', cat_loss_type = 'categorical'):\n",
    "        self.n_disc = n_disc\n",
    "        self.lambda_cls = lambda_cls\n",
    "        self.lambda_rec = lambda_rec\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5, beta_2 = 0.999)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5, beta_2 = 0.999)\n",
    "        \n",
    "        if callable(gan):\n",
    "            if config is not None:\n",
    "                self.gan = gan(config)\n",
    "            else:\n",
    "                raise Exception('missing argument `config`')\n",
    "        else:\n",
    "            self.gan = gan\n",
    "            \n",
    "        self.generator = gan.generator\n",
    "        self.discriminator = gan.discriminator\n",
    "        \n",
    "        self.losses = Losses(cat_loss_type)\n",
    "        \n",
    "    def train(self, data, epochs = 1):\n",
    "        gen_losses, disc_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print(f'Epoch: {e} Starts')\n",
    "            for real_img, real_cls, tar_img, tar_cls in data:\n",
    "                gen_loss, disc_loss = self.train_step(real_img, real_cls, tar_img, tar_cls)\n",
    "                print('.', end = '')\n",
    "                \n",
    "            gen_losses.append(gen_loss)\n",
    "            disc_losses.append(disc_loss)\n",
    "            print(f'\\nGenerator Loss: {gen_loss} \\t Discriminator Loss: {disc_loss}')\n",
    "            print(f'Epoch: {e} Ends\\n')\n",
    "            \n",
    "        return {'gen_losses': gen_losses, 'disc_losses': disc_losses}\n",
    "        \n",
    "    def generator_loss(self, disc_gen_out_src, real_img, re_gen_img, disc_gen_out_cls, tar_cls, loss_type = 'wgan'):\n",
    "        if self.loss_type == 'wgan':\n",
    "            gen_loss = self.losses.gen_wgan_loss(disc_gen_out_src)\n",
    "        elif self.loss_type == 'adverserial':\n",
    "            gen_loss = self.losses.adversarial_loss_gen(disc_gen_out_src)\n",
    "        \n",
    "        recon_loss = self.losses.reconstruction_loss(real_img, re_gen_img)\n",
    "        gen_cls_loss = self.losses.cls_loss(tar_cls, disc_gen_out_cls)\n",
    "        \n",
    "        loss = gen_loss + self.lambda_cls * gen_cls_loss + self.lambda_rec * recon_loss\n",
    "        return loss\n",
    "    \n",
    "    def discriminator_loss(self, tar_img, real_cls, gen_out, disc_real_out_src, disc_gen_out_src, disc_real_out_cls, \n",
    "                           loss_type = 'wgan'):\n",
    "        if self.loss_type == 'wgan':\n",
    "            disc_loss = self.losses.disc_wgan_loss(disc_real_out_src, disc_gen_out_src)\n",
    "            disc_loss += self.losses.gradeint_penalty(self.discriminator, tar_img, gen_out)\n",
    "        elif self.loss_type == 'adverserial':\n",
    "            disc_loss = self.losses.adversarial_loss_gen(disc_real_out_src, disc_gen_out_src)\n",
    "        \n",
    "        disc_cls_loss = self.losses.cls_loss(real_cls, disc_real_out_cls)\n",
    "        \n",
    "        loss = disc_loss + self.lambda_cls * disc_cls_loss\n",
    "        return loss\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, real_img, real_cls, tar_img, tar_cls):\n",
    "        \n",
    "        for _ in range(self.n_disc):\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                gen_out = self.generator([real_img, tar_cls], training = True)\n",
    "                \n",
    "                disc_real_out_src, disc_real_out_cls = self.discriminator(real_img, training = True)\n",
    "                disc_gen_out_src, disc_gen_out_cls = self.discriminator(gen_out, training = True)\n",
    "                \n",
    "                disc_loss = self.discriminator_loss(tar_img, real_cls, gen_out, disc_real_out_src, \n",
    "                                                    disc_gen_out_src, disc_real_out_cls)\n",
    "                \n",
    "            disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.disc_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "            \n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_out = self.generator([real_img, tar_cls], training = True)\n",
    "            re_gen_out = self.generator([gen_out, real_cls], training = True)\n",
    "\n",
    "            disc_gen_out_src, disc_gen_out_cls = self.discriminator(gen_out, training = True)\n",
    "                \n",
    "            gen_loss = self.generator_loss(disc_gen_out_src, real_img, re_gen_out, disc_gen_out_cls, tar_cls)  \n",
    "        \n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        return disc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629e16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri, rl = tf.random.uniform((120, 128, 128, 3)), tf.random.uniform((120, 3))\n",
    "ti, tl = tf.random.uniform((120, 128, 128, 3)), tf.random.uniform((120, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbcadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ri, rl, ti, tl)).shuffle(1200).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ebff822",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainer(GAN(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e813577",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Starts\n",
      "............................................................\n",
      "Generator Loss: -2172282.25 \t Discriminator Loss: 27368048.0\n",
      "Epoch: 0 Ends\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gen_losses': [<tf.Tensor: shape=(), dtype=float32, numpy=-2172282.2>],\n",
       " 'disc_losses': [<tf.Tensor: shape=(), dtype=float32, numpy=27368048.0>]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.train(dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d117c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
