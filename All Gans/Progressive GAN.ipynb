{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdddcf8",
   "metadata": {},
   "source": [
    "# Progressive Gan\n",
    "\n",
    "This is an attempt to re-implement the paper Progressive gan\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1710.10196.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://github.com/tkarras/progressive_growing_of_gans\n",
    "* https://github.com/fabulousjeong/pggan-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81811c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.math.rsqrt(tf.math.reduce_mean(tf.square(inputs), axis = -1, keepdims = True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStddev(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon = 1e-8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inp_shp = tf.shape(inputs)\n",
    "        \n",
    "        mean = tf.math.reduce_mean(inputs, axis = 0, keepdims = True)\n",
    "        std = tf.math.sqrt(tf.math.reduce_mean(tf.square(inputs - mean), axis = 0, keepdims = True) + self.epsilon)\n",
    "        avg_std = tf.math.reduce_mean(std, keepdims = True)\n",
    "        tiled = tf.tile(avg_std, (inp_shp[0], inp_shp[1], inp_shp[2], 1))\n",
    "        combined = tf.concat([inputs, tiled], axis = -1)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d979f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, gain = np.sqrt(2), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gain = gain\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        inp_chn = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (inp_chn, self.units), initializer = init, \n",
    "                                 trainable = True, name = 'Weight')\n",
    "        self.B = self.add_weight(shape = (self.units, ), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'Bias')\n",
    "        \n",
    "        self.w_scale = self.gain * tf.math.rsqrt(tf.cast(inp_chn, tf.float32))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.matmul(inputs, self.W * self.w_scale), self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.gain = gain\n",
    "        self.padding = 'SAME' if (kernel_size[0]-1)//2 else 'VALID'\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        inp_chn = input_shape[-1]\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0)\n",
    "        self.W = self.add_weight(shape = (self.kernel_size[0], self.kernel_size[1], inp_chn, self.filters), \n",
    "                                 initializer = init, trainable = True, name = 'Weight')\n",
    "        self.B = self.add_weight(shape = (self.filters, ), initializer = 'zeros', \n",
    "                                 trainable = True, name = 'Bias')\n",
    "        \n",
    "        fan_in = tf.cast(self.kernel_size[0] * self.kernel_size[1] * inp_chn, tf.float32)\n",
    "        self.w_scale = self.gain * tf.math.rsqrt(fan_in)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.add(tf.nn.conv2d(inputs, self.W * self.w_scale, self.strides, self.padding, data_format = 'NHWC'), self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(tf.keras.layers.Layer):\n",
    "    def __init__(self, alpha = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if alpha is not None: \n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = tf.Variable(0.0, name = 'ws_alpha')\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        return ((1 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2596cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [512, 512, 512, 512, 256, 128, 64, 32, 16]\n",
    "class ProgressiveGAN(tf.keras.models.Model):\n",
    "    def __init__(self, latent_dim = 512, d_steps = 1, gp_weight = 10, drift_weight = 0.001):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_steps = d_steps\n",
    "        self.drift_weight = drift_weight\n",
    "        \n",
    "        self.alpha = tf.Variable(0.0, 'ws_alpha') # for temporary\n",
    "        self.num_block = 2\n",
    "        \n",
    "        self.generator = self.__init_generator\n",
    "        self.discriminator = self.__init_discriminator\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def __init_generator(self):\n",
    "        res = 2**self.num_block\n",
    "        \n",
    "        inp = tf.keras.layers.Input(shape = (1, 1, self.latent_dim), dtype = tf.float32, name = 'Latent_input')\n",
    "        x = PixelNormalization()(inp)\n",
    "        \n",
    "        x = Dense(units = res*res*self.latent_dim, gain = np.sqrt(2)/res)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = PixelNormalization()(x)\n",
    "        x = tf.keras.layers.Reshape((res, res, self.latent_dim))(x)\n",
    "        \n",
    "        x = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (4, 4), strides = (1, 1), gain = np.sqrt(2))(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = PixelNormalization()(x)\n",
    "        \n",
    "        x = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = PixelNormalization()(x)\n",
    "        \n",
    "        x = Conv2D(filters = 3, kernel_size = (1, 1), strides = (1, 1), gain = 1.0, name = f'to_rgb_conv_{res}x{res}')(x)\n",
    "        x = tf.keras.layers.Activation('tanh', name = f'to_rgb_act_{res}x{res}')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, x, name = f'generator_{res}x{res}')\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def __grow_generator(self):\n",
    "        res = 2**self.num_block\n",
    "        prev_res = 2**(self.num_block-1)\n",
    "        for n, layer in enumerate(self.generator.layers):\n",
    "            if layer.name == f'to_rgb_conv_{prev_res}x{prev_res}':\n",
    "                to_rgb_conv = layer\n",
    "                t = n - 1\n",
    "            elif layer.name == f'to_rgb_act_{prev_res}x{prev_res}':\n",
    "                to_rgb_act = layer\n",
    "                \n",
    "                \n",
    "        end_block = self.generator.layers[t].output\n",
    "        up_sample = tf.keras.layers.UpSampling2D()(end_block)\n",
    "        \n",
    "        x1 = to_rgb_conv(up_sample)\n",
    "        x1 = to_rgb_act(x1)\n",
    "        \n",
    "        x2 = Conv2D(filters = FILTERS[self.num_block-2], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(up_sample)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(x2)\n",
    "        x2 = PixelNormalization()(x2)\n",
    "        \n",
    "        x2 = Conv2D(filters = FILTERS[self.num_block-2], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(x2)\n",
    "        x2 = PixelNormalization()(x2)\n",
    "        \n",
    "        x2 = Conv2D(filters = 3, kernel_size = (1, 1), strides = (1, 1), gain = 1.0, name = f'to_rgb_conv_{res}x{res}')(x2)\n",
    "        # x2 = tf.keras.layers.LeakyReLU(alpha = 0.2, name = f'to_rgb_act_{res}x{res}')(x2)\n",
    "        x2 = tf.keras.layers.Activation('tanh', name = f'to_rgb_act_{res}x{res}')(x2)\n",
    "        \n",
    "        x = WeightedSum(self.alpha)([x1, x2])\n",
    "        \n",
    "        self.generator = tf.keras.models.Model(self.generator.inputs, x, name = f'generator_{res}x{res}')\n",
    "        self.stabilized_generator = tf.keras.models.Model(self.generator.inputs, x2, name = f'stabilized_generator_{res}x{res}')\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def __init_discriminator(self):\n",
    "        res = 2**self.num_block\n",
    "        inp = tf.keras.layers.Input(shape = (res, res, 3), name = f'discriminator_input_{res}x{res}')\n",
    "        \n",
    "        x = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (1, 1), strides = (1, 1), gain = np.sqrt(2), \n",
    "                   name = f'from_rgb_conv_{res}x{res}')(inp)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2, name = f'from_rgb_act_{res}x{res}')(x)\n",
    "        x = MinibatchStddev()(x)\n",
    "        \n",
    "        x = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        x = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (4, 4), strides = (4, 4), gain = np.sqrt(2))(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = Dense(units = 1, gain = 1.0)(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, x, name = f'discriminator_{res}x{res}')\n",
    "    \n",
    "    @property\n",
    "    def __grow_discriminator(self):\n",
    "        res = 2**self.num_block\n",
    "        prev_res = 2**(self.num_block - 1)\n",
    "        \n",
    "        inp = tf.keras.layers.Input(shape = (res, res, 3), dtype = tf.float32, name = f'discriminator_input_{res}x_{res}')\n",
    "        \n",
    "        for n, layer in enumerate(self.discriminator.layers):\n",
    "            if layer.name == f'from_rgb_conv_{prev_res}x{prev_res}':\n",
    "                from_rgb_conv = layer\n",
    "            elif layer.name == f'from_rgb_act_{prev_res}x{prev_res}':\n",
    "                from_rgb_act = layer\n",
    "                t = n + 1\n",
    "                \n",
    "        x1 = tf.keras.layers.AveragePooling2D()(inp)\n",
    "        x1 = from_rgb_conv(x1)\n",
    "        x1 = from_rgb_act(x1)\n",
    "        \n",
    "        x2 = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (1, 1), strides = (1, 1), gain = np.sqrt(2), \n",
    "                    name = f'from_rgb_conv_{res}x{res}')(inp)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha = 0.2, name = f'from_rgb_act_{res}x{res}')(x2)\n",
    "        \n",
    "        x2 = Conv2D(filters = FILTERS[self.num_block - 2], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(x2)\n",
    "        x2 = Conv2D(filters = FILTERS[self.num_block - 3], kernel_size = (3, 3), strides = (1, 1), gain = np.sqrt(2))(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(x2)\n",
    "        \n",
    "        x2 = tf.keras.layers.AveragePooling2D()(x2)\n",
    "        \n",
    "        x = WeightedSum(self.alpha)([x1, x2])\n",
    "        \n",
    "        for i in range(t, len(self.discriminator.layers)):\n",
    "            x = self.discriminator.layers[i](x)\n",
    "            x2 = self.discriminator.layers[i](x2)\n",
    "            \n",
    "        self.discriminator = tf.keras.models.Model(inp, x, name = f'discriminator_{res}x{res}')\n",
    "        self.stabilized_discriminator = tf.keras.models.Model(inp, x2, name = f'stabilized_discriminator_{res}x{res}')\n",
    "                \n",
    "    \n",
    "    def grow_model(self):\n",
    "        self.num_block += 1\n",
    "        self.__grow_generator\n",
    "        self.__grow_discriminator\n",
    "    \n",
    "    def stabilize_model(self):\n",
    "        self.generator = self.stabilized_generator\n",
    "        self.discriminator = self.stabilized_discriminator\n",
    "    \n",
    "    def generator_loss(self, disc_gen_out):\n",
    "        return -tf.reduce_mean(disc_gen_out)\n",
    "    \n",
    "    def gradient_penalty(self, real_img, gen_img):\n",
    "        #print(real_img.shape, gen_img.shape)\n",
    "        #assert tf.shape(real_img) == tf.shape(gen_img)\n",
    "        batch_size = tf.shape(real_img)[0]\n",
    "        \n",
    "        epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
    "        interpolated_img = ((1 - epsilon) * real_img) + (epsilon * gen_img)\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_img)\n",
    "            pred = self.discriminator(interpolated_img)\n",
    "        grads = gp_tape.gradient(pred, [interpolated_img])[0]\n",
    "        norm = tf.math.sqrt(tf.math.reduce_mean(tf.square(grads), axis = [1, 2, 3], keepdims = True))\n",
    "        gp = tf.math.reduce_mean(tf.square(norm - 1))\n",
    "        return gp * self.gp_weight\n",
    "    \n",
    "    def discriminator_loss(self, disc_real_out, disc_gen_out):\n",
    "        return tf.math.reduce_mean(disc_gen_out) - tf.math.reduce_mean(disc_real_out)\n",
    "    \n",
    "    def drift_loss(self, disc_real_out):\n",
    "        return tf.math.reduce_mean(tf.square(disc_real_out)) * self.drift_weight\n",
    "    \n",
    "    def compile(self, optimizer):\n",
    "        super(ProgressiveGAN, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        \n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        latent_input = tf.random.normal((batch_size, 1, 1, self.latent_dim))\n",
    "        for i in range(self.d_steps):\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                gen_out = self.generator(latent_input, training = True)\n",
    "                \n",
    "                disc_real_out = self.discriminator(real_images, training = True)\n",
    "                disc_gen_out = self.discriminator(gen_out, training = True)\n",
    "                \n",
    "                gp = self.gradient_penalty(real_images, gen_out)\n",
    "                drf_loss = self.drift_loss(disc_real_out)\n",
    "                disc_loss = self.discriminator_loss(disc_real_out, disc_gen_out) + gp + drf_loss\n",
    "                \n",
    "            disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "            \n",
    "        latent_input = tf.random.normal((batch_size, 1, 1, self.latent_dim))\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_out = self.generator(latent_input, training = True)\n",
    "            disc_gen_out = self.discriminator(gen_out, training = True)\n",
    "            gen_loss = self.generator_loss(disc_gen_out)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        return {'disc_loss': disc_loss, 'gen_loss': gen_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.n_epoch = 0\n",
    "    \n",
    "    def set_steps(self, steps_per_epoch, epochs):\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.steps = self.steps_per_epoch * self.epochs\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        self.n_epoch = epoch\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs = None):\n",
    "        alpha = ((self.n_epoch * self.steps_per_epoch) + batch) / float(self.steps - 1)\n",
    "        self.model.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2d74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pgan = ProgressiveGAN()\n",
    "\n",
    "data_path = r'E:\\Image Datasets\\Celeb A\\Dataset\\img_align_celeba'\n",
    "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = lambda x: (tf.cast(x, tf.float32)/127.5) - 1)\n",
    "\n",
    "BATCHES = [4, 4, 4, 4, 1, 1, 1, 1, 1]\n",
    "EPOCHS = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# steps_per_epoch = [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
    "steps_per_epoch = [5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
    "models = {}\n",
    "\n",
    "cbk = Callback()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.0, beta_2 = 0.99, epsilon = 1e-8)\n",
    "\n",
    "for i in range(9):\n",
    "    res = 2**(i+2)\n",
    "    models[f'{res}x{res}'] = {}\n",
    "    train_data = train_image_generator.flow_from_directory(batch_size = BATCHES[i], directory = data_path, shuffle = True, \n",
    "                                                           target_size = (res, res), class_mode = 'binary')\n",
    "    if i != 0:\n",
    "        pgan.grow_model()\n",
    "    \n",
    "    cbk.set_steps(steps_per_epoch[i], EPOCHS[i])\n",
    "    pgan.compile(optimizer)\n",
    "    pgan.fit(train_data, steps_per_epoch = steps_per_epoch[i], epochs = EPOCHS[i], callbacks = [cbk])\n",
    "    \n",
    "    models[f'{res}x{res}']['main'] = {}\n",
    "    models[f'{res}x{res}']['main']['generator'] = pgan.generator\n",
    "    models[f'{res}x{res}']['main']['discriminator'] = pgan.discriminator\n",
    "\n",
    "    if i != 0:\n",
    "        pgan.stabilize_model()\n",
    "        \n",
    "        pgan.compile(optimizer)\n",
    "        pgan.fit(train_data, steps_per_epoch = steps_per_epoch[i], epochs = EPOCHS[i], callbacks = [cbk])\n",
    "    \n",
    "        models[f'{res}x{res}']['stabilized'] = {}\n",
    "        models[f'{res}x{res}']['stabilized']['generator'] = pgan.generator\n",
    "        models[f'{res}x{res}']['stabilized']['discriminator'] = pgan.discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e67436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 512\n",
    "plt.imshow(((models[f'{r}x{r}']['main']['generator'](tf.random.normal((1, 1, 1, 512)))[0]+1)*127.5).numpy().astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442194d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan = ProgressiveGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.grow_model()\n",
    "# gan.stabilize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.stabilize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66f85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(gan.generator, show_shapes = True, dpi = 64)\n",
    "# tf.keras.utils.plot_model(gan.discriminator, 'model2.png', show_shapes = True, dpi = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2e953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gan.discriminator.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
