{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147501d0",
   "metadata": {},
   "source": [
    "# VQ-VAE (Vector Quantized - Variational AutoEncoder)\n",
    "\n",
    "This is an attempt to re-implement the paper AD-GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1711.00937.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://keras.io/examples/generative/vq_vae\n",
    "* https://www.youtube.com/watch?v=VZFVUrYcig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effafe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6cf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = tf.cast(tf.expand_dims(x_train, -1), tf.float32)\n",
    "x_test = tf.cast(tf.expand_dims(x_test, -1), tf.float32)\n",
    "x_train_scaled = (x_train / 255.0)\n",
    "x_test_scaled = (x_test / 255.0)\n",
    "\n",
    "inp_shape = x_train_scaled.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e122297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f7fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_embedding, embedding_dim, \n",
    "                 commitment_cost = 0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embedding = num_embedding\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        init = tf.random_uniform_initializer()\n",
    "        self.codebook = tf.Variable(\n",
    "            initial_value = init(\n",
    "                shape = (self.num_embedding, self.embedding_dim), dtype = tf.float32\n",
    "            ), trainable = True, name = 'embeddings_vqvae',\n",
    "        )\n",
    "        \n",
    "    def call(self, z):\n",
    "        z_shape = tf.shape(z)\n",
    "        flat_z = tf.reshape(z, (-1, self.embedding_dim))\n",
    "        \n",
    "        # distance between z and codebook\n",
    "        d = (\n",
    "            tf.math.reduce_sum(flat_z ** 2, axis = 1, keepdims = True)  # sum over the axis of embed_dim\n",
    "            + tf.math.reduce_sum(self.codebook ** 2, axis = 1)          # sum over the axis of embed_dim\n",
    "            - 2 * tf.matmul(flat_z, self.codebook, transpose_b = True)\n",
    "        )\n",
    "        \n",
    "        # Quantization\n",
    "        embed_indices = tf.argmin(d, axis = 1)\n",
    "        embeds = tf.one_hot(embed_indices, self.num_embedding)\n",
    "        embeds = tf.matmul(embeds, self.codebook)\n",
    "        quantized = tf.reshape(embeds, z_shape)\n",
    "        \n",
    "        # losses\n",
    "        loss = tf.math.reduce_mean(\n",
    "            (tf.stop_gradient(z) - quantized) ** 2\n",
    "        ) + self.commitment_cost * tf.math.reduce_mean(\n",
    "            (z - tf.stop_gradient(quantized)) ** 2\n",
    "        )\n",
    "        \n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # straight-through estimator\n",
    "        quantized = z + tf.stop_gradient(quantized - z)\n",
    "        return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_decoder(inp_size, dim, latent_dim, num_residual = 0, apply_out_sigmoid = True):\n",
    "    \n",
    "    def residual_block(x, d):\n",
    "            r = tf.keras.layers.Conv2D(filters = d, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "            r = tf.keras.layers.ReLU()(r)\n",
    "            r = tf.keras.layers.Conv2D(filters = d, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(r)\n",
    "            r = tf.keras.layers.ReLU()(r)\n",
    "            return tf.keras.layers.Add()([r, x])\n",
    "        \n",
    "    # Encoder\n",
    "    enc_inp = tf.keras.layers.Input(shape = inp_size, dtype = 'float32')\n",
    "    enc = tf.keras.layers.Conv2D(filters = dim // 2, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(enc_inp)\n",
    "    enc = tf.keras.layers.ReLU()(enc)\n",
    "    enc = tf.keras.layers.Conv2D(filters = dim, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(enc)\n",
    "    enc = tf.keras.layers.ReLU()(enc)\n",
    "    for _ in range(num_residual):\n",
    "        enc = residual_block(enc, dim)\n",
    "    enc = tf.keras.layers.Conv2D(latent_dim, kernel_size = (1, 1), strides = (1, 1), padding = 'same')(enc)\n",
    "    \n",
    "    encoder = tf.keras.models.Model(enc_inp, enc, name = 'Encoder')\n",
    "        \n",
    "    # Decoder \n",
    "    dec_inp = tf.keras.layers.Input(shape = encoder.output_shape[1:], dtype = 'float32')\n",
    "    dec = dec_inp\n",
    "    for _ in range(num_residual):\n",
    "        dec = residual_block(dec, latent_dim)\n",
    "    dec = tf.keras.layers.Conv2DTranspose(filters = dim, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(dec)\n",
    "    dec = tf.keras.layers.ReLU()(dec)\n",
    "    dec = tf.keras.layers.Conv2DTranspose(filters = dim // 2, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(dec)\n",
    "    dec = tf.keras.layers.ReLU()(dec)\n",
    "    dec = tf.keras.layers.Conv2DTranspose(filters = inp_size[-1], kernel_size = (3, 3), strides = (1, 1), padding = 'same')(dec)\n",
    "\n",
    "    if apply_out_sigmoid:\n",
    "        dec = tf.keras.layers.Activation('sigmoid')(dec)\n",
    "    decoder = tf.keras.models.Model(dec_inp, dec, name = 'Decoder')\n",
    "    \n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5884a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(tf.keras.models.Model):\n",
    "    def __init__(self, inp_size, num_embedding = 128, latent_dim = 256, dim = 64, num_residual = 0, \n",
    "                 apply_out_sigmoid = True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vqvae = self.__vqvae_model(inp_size, num_embedding, latent_dim, dim, num_residual, apply_out_sigmoid)\n",
    "        \n",
    "    def __vqvae_model(self, inp_size, num_embedding, latent_dim, dim = 64, num_residual = 0, apply_out_sigmoid = True):\n",
    "        encoder, decoder = get_encoder_decoder(inp_size, dim, latent_dim)\n",
    "        vqvae_inp = tf.keras.layers.Input(shape = inp_size, dtype = tf.float32)\n",
    "        vq_vae = encoder(vqvae_inp)\n",
    "        vq_vae = VectorQuantizer(num_embedding, latent_dim)(vq_vae)\n",
    "        vq_vae = decoder(vq_vae)\n",
    "        return tf.keras.models.Model(vqvae_inp, vq_vae, name = 'VQVAE')\n",
    "    \n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.vqvae.summary()\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            vqvae_out = self.vqvae(x)\n",
    "            \n",
    "            reconstruction_loss = tf.math.reduce_mean(tf.math.square(x - vqvae_out))\n",
    "            total_loss = reconstruction_loss + tf.math.reduce_sum(self.vqvae.losses)\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "        \n",
    "        return {'total_loss': total_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19660a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 [==============================] - 6s 9ms/step - total_loss: 0.2346A: 0s - tota\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0476\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 4s 8ms/step - total_loss: 0.0311A: 0s - total_loss: \n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0242\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0215\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0204A: 0s - total_loss:\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0199A: 1s - total_loss - ETA: 1s \n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0190\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0187A: 0s - total_los\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0182A: 1\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0178\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0174\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0169\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0165\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0161\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0157\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0155\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0153A: 1s - total_lo - ETA: 0s - total_\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0151\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0150\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0149\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0147\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0145\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0144A: 0s - total_loss: 0.\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0143A: 1s - total_loss: - ETA: 0s - total\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0142A: 3s - total - ETA: 2s - total_loss: 0. - ETA: 2s - total_los\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0142A: 3s - total_loss - ETA: 2s - \n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 4s 9ms/step - total_loss: 0.0141A: 3s - total_lo - ETA: 3 - ETA: 0s - total_loss: 0\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 8s 17ms/step - total_loss: 0.0140\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 19s 40ms/step - total_loss: 0.0140\n"
     ]
    }
   ],
   "source": [
    "vqvae = VQVAE(inp_shape)\n",
    "vqvae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "losses = vqvae.fit(x_train_scaled, epochs = 30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "839c1f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x2160 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 30))\n",
    "num_examples = 1\n",
    "\n",
    "for i in range(num_examples):\n",
    "    idx = np.random.choice(np.arange(10000))\n",
    "    orig_img = tf.expand_dims(x_test_scaled[idx], 0)\n",
    "    recon_img = vqvae.vqvae(orig_img)\n",
    "\n",
    "    plt.subplot(num_examples, 2, 2*i + 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(orig_img[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(num_examples, 2, 2*i + 2)\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.imshow(recon_img[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771f810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
