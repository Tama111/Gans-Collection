{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac422c73",
   "metadata": {},
   "source": [
    "# DC-GAN (Deep Convolutional Gan)\n",
    "\n",
    "This is an attempt to re-implement the paper DC-GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1511.06434v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d846172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 256\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8609080",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21314d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset_main = tf.data.Dataset.from_tensor_slices(main_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84469511",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_dataset_main, len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab9cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_size = 100):\n",
    "    \n",
    "    init = tf.random_normal_initializer(0, 0.02)\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape = (latent_size, ))\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units = 4 * 4 * 1024, activation = tf.nn.relu, kernel_initializer = init)(inp)\n",
    "    x = tf.keras.layers.Reshape((4, 4, 1024))(x)\n",
    "    \n",
    "    x =  tf.keras.layers.Conv2D(filters = 1024, kernel_size = (3, 3), strides = (1, 1), \n",
    "                                  padding = 'same', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 512, kernel_size = (3, 3), strides = (2, 2), \n",
    "                                          padding = 'same', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = (3, 3), strides = (2, 2), \n",
    "                                          padding = 'same', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (5, 5), strides = (2, 2), \n",
    "                                          padding = 'same', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), \n",
    "                                padding = 'valid', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), \n",
    "                                padding = 'valid', kernel_initializer = init)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs = inp, outputs = x, name = 'Generator')\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    init = tf.random_normal_initializer(0, 0.02)\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape = (28, 28, 1))\n",
    "    \n",
    "    m = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (2, 2), \n",
    "                                padding = 'same', kernel_initializer = init)(inp)\n",
    "    m = tf.keras.layers.BatchNormalization()(m)\n",
    "    m = tf.keras.layers.LeakyReLU(alpha = 0.2)(m)\n",
    "    \n",
    "    m = tf.keras.layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (2, 2), \n",
    "                               padding = 'same', kernel_initializer = init)(m)\n",
    "    m = tf.keras.layers.BatchNormalization()(m)\n",
    "    m = tf.keras.layers.LeakyReLU(alpha = 0.2)(m)\n",
    "    \n",
    "    m = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (2, 2), \n",
    "                                 padding = 'same', kernel_initializer = init)(m)\n",
    "    m = tf.keras.layers.BatchNormalization()(m)\n",
    "    m = tf.keras.layers.LeakyReLU(alpha = 0.2)(m)\n",
    "    \n",
    "    f1 = tf.keras.layers.Flatten()(m)\n",
    "    out = tf.keras.layers.Dense(units = 1, kernel_initializer = init)(f1)\n",
    "    \n",
    "    return tf.keras.Model(inputs = inp, outputs = out)\n",
    "\n",
    "gen = generator()\n",
    "disc = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe78074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "def discriminator_loss(disc_real_out, disc_gen_out):\n",
    "    real_loss = loss_function(tf.ones_like(disc_real_out), disc_real_out)\n",
    "    gen_loss = loss_function(tf.zeros_like(disc_gen_out), disc_gen_out)\n",
    "    \n",
    "    total_loss = real_loss + gen_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(disc_gen_out):\n",
    "    loss = loss_function(tf.ones_like(disc_gen_out), disc_gen_out)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964db22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15238724",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image):\n",
    "    latent_space = tf.random.normal([BATCH_SIZE, 100])\n",
    "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "        gen_out = gen(latent_space, training = True)\n",
    "        \n",
    "        disc_real_out = disc(real_image, training = True)\n",
    "        disc_gen_out = disc(gen_out, training = True)\n",
    "        \n",
    "        disc_loss = discriminator_loss(disc_real_out, disc_gen_out)\n",
    "        gen_loss = generator_loss(disc_gen_out)\n",
    "        \n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc.trainable_variables)\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    \n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc.trainable_variables))\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72607122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_space(size):\n",
    "    return np.random.normal(0, 2, size = (size, 100))\n",
    "\n",
    "images = []\n",
    "def train(dataset, epochs = 1):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for n, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            \n",
    "            if n%10==0: \n",
    "                print('.', end = '')\n",
    "            \n",
    "        inp = sample_latent_space(1)[0].reshape(1, 100)\n",
    "        x = gen(inp).numpy()[0]\n",
    "        images.append(x)\n",
    "        print (f'\\nGenerator:- {gen_loss} Discriminator:- {disc_loss}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67e153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_dataset, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4f2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f7424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
