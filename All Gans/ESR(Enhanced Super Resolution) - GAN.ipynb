{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6d1176",
   "metadata": {},
   "source": [
    "# ESR-GAN (Enhanced Super Resolution Gan)\n",
    "\n",
    "This is an attempt to re-implement the paper ESR-GAN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1809.00219.pdf\n",
    "\n",
    "Other Resources: \n",
    "* https://www.youtube.com/watch?v=qwYOlXRdADI\n",
    "* https://github.com/xinntao/ESRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30d1e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7f0a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters = 32, filters_l = 64, kernel_size = (3, 3), strides = (1, 1), \n",
    "                 padding = 'same', beta = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.conv0 = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, \n",
    "                                            padding = padding)\n",
    "        self.lrelu0 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.concat1 = tf.keras.layers.Concatenate()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, \n",
    "                                            padding = padding)\n",
    "        self.lrelu1 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.concat2 = tf.keras.layers.Concatenate()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, \n",
    "                                            padding = padding)\n",
    "        self.lrelu2 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.concat3 = tf.keras.layers.Concatenate()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, \n",
    "                                            padding = padding)\n",
    "        self.lrelu3 = tf.keras.layers.LeakyReLU(alpha = 0.2)\n",
    "        \n",
    "        self.concat4 = tf.keras.layers.Concatenate()\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters = filters_l, kernel_size = kernel_size, strides = strides, \n",
    "                                            padding = padding)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x0 = self.lrelu0(self.conv0(inputs))\n",
    "        x1 = self.lrelu1(self.conv1(self.concat1([inputs, x0])))\n",
    "        x2 = self.lrelu2(self.conv2(self.concat2([inputs, x0, x1])))\n",
    "        x3 = self.lrelu3(self.conv3(self.concat3([inputs, x0, x1, x2])))\n",
    "        x4 = self.conv4(self.concat4([inputs, x0, x1, x2, x3]))\n",
    "        \n",
    "        return x4 * self.beta + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4850e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRDB(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters = 32, filters_l = 64, beta = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.RDB1 = ResidualDenseBlock(filters = filters, filters_l = filters_l)\n",
    "        self.RDB2 = ResidualDenseBlock(filters = filters, filters_l = filters_l)\n",
    "        self.RDB3 = ResidualDenseBlock(filters = filters, filters_l = filters_l)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        out = self.RDB1(inputs)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out * self.beta + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b6f326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inp_shape, filters = 32, filters_l = 64, num_blocks = 23):\n",
    "    inp = tf.keras.layers.Input(shape = inp_shape, dtype = tf.float32, name = 'generator_input')\n",
    "    \n",
    "    t = tf.keras.layers.Conv2D(filters = filters_l, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(inp)\n",
    "    \n",
    "    x = t\n",
    "    for _ in range(num_blocks):\n",
    "        x = RRDB(filters = filters, filters_l = filters_l)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = filters_l, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.Add()([x, t])\n",
    "    \n",
    "    for _ in range(2):\n",
    "        x = tf.keras.layers.UpSampling2D(size = (2, 2), interpolation = 'nearest')(x)\n",
    "        x = tf.keras.layers.Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(filters = filters, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters = inp_shape[-1], kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    \n",
    "    return tf.keras.models.Model(inp, x, name = 'Generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f91cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gen = generator((64, 64, 3))\n",
    "# gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a6dffe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inp_shape):\n",
    "    inp = tf.keras.layers.Input(shape = inp_shape, dtype = tf.float32, name = f'discriminator_input_{inp_shape}')\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(inp)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(units = 1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha = 0.2)(x)\n",
    "    x = tf.keras.layers.Dense(units = 1)(x)\n",
    "    #x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.models.Model(inp, x, name = 'Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47f2bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# disc = discriminator(gen.output_shape[1:])\n",
    "# disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9939dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_model(out_layer = 'block5_conv4', act_off = True):\n",
    "    vgg_model = tf.keras.applications.VGG19(include_top = False, weights = 'imagenet')\n",
    "    vgg_model.trainable = False\n",
    "    \n",
    "    out = vgg_model.get_layer(out_layer).output\n",
    "\n",
    "    \n",
    "    model = tf.keras.models.Model(vgg_model.inputs, out, name = 'vgg_model')\n",
    "    if act_off:\n",
    "        model.layers[-1].activation = None\n",
    "        return model\n",
    "    return model\n",
    "vgg_model = get_vgg_model(act_off = True)\n",
    "vgg_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cdc44c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_loss(real, gen, rescale_factor = 1/12.75):\n",
    "    # real and gen_img, pixel range [-1, 1]\n",
    "    preprocessed_real = tf.keras.applications.vgg19.preprocess_input((real + 1)*127.5) * rescale_factor\n",
    "    preprocessed_gen = tf.keras.applications.vgg19.preprocess_input((gen + 1)*127.5) * rescale_factor\n",
    "    \n",
    "    vgg_real = vgg_model(preprocessed_real)\n",
    "    vgg_gen = vgg_model(preprocessed_gen)\n",
    "    \n",
    "    return tf.math.reduce_mean(tf.math.square(vgg_real - vgg_gen))\n",
    "\n",
    "def l1_loss(real, gen):\n",
    "    return tf.math.reduce_mean(tf.abs(real - gen))\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "def relativistic_generator_loss(disc_real_out, disc_gen_out):\n",
    "    real_r = tf.nn.sigmoid(disc_real_out - tf.math.reduce_mean(disc_gen_out, axis = 0, keepdims = True))\n",
    "    real_loss = bce(tf.zeros_like(real_r), real_r)\n",
    "    \n",
    "    gen_r = tf.nn.sigmoid(disc_gen_out - tf.math.reduce_mean(disc_real_out, axis = 0, keepdims = True))\n",
    "    gen_loss = bce(tf.ones_like(gen_r), gen_r)\n",
    "    return real_loss + gen_loss\n",
    "    \n",
    "def relativistic_discriminator_loss(disc_real_out, disc_gen_out):\n",
    "    real_r = tf.nn.sigmoid(disc_real_out - tf.math.reduce_mean(disc_gen_out, axis = 0, keepdims = True))\n",
    "    real_loss = bce(tf.ones_like(real_r), real_r)\n",
    "    \n",
    "    gen_r = tf.nn.sigmoid(disc_gen_out - tf.math.reduce_mean(disc_real_out, axis = 0, keepdims = True))\n",
    "    gen_loss = bce(tf.zeros_like(gen_r), gen_r)\n",
    "    return real_loss + gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9723cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "disc_opt = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "db7f75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(lr_img, hr_img):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_out = gen(lr_img, training = True)\n",
    "        \n",
    "        disc_real_out = disc(hr_img, training = True)\n",
    "        disc_gen_out = disc(gen_out, training = True)\n",
    "        \n",
    "        disc_loss = relativistic_discriminator_loss(disc_real_out, disc_gen_out)\n",
    "        gen_loss = (relativistic_generator_loss(disc_real_out, disc_gen_out) * 5e-3 + n * l1_loss(hr_img, gen_out) * 1e-2 \n",
    "                    + vgg_loss(hr_img, gen_out))\n",
    "        \n",
    "    gen_grads = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gen_grads, gen.trainable_variables))\n",
    "    \n",
    "    disc_grads = disc_tape.gradient(disc_loss, dics.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(disc_grads, disc.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "de5205c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs = 1):\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch: {e} Starts')\n",
    "        for lr_img, hr_img in data:\n",
    "            gen_loss, disc_loss = train_step(lr_img, hr_img)\n",
    "            print('.', end='')\n",
    "            \n",
    "        print(f'\\nGenerator Loss: {gen_loss} \\t Discriminator Loss: {disc_loss}')\n",
    "        print(f'Epoch: {e} Ends\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
